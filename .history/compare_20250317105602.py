Model keys: odict_keys(['model.image_encoder.patch_embed.seq.0.c.weight', 'model.image_encoder.patch_embed.seq.0.bn.weight', 'model.image_encoder.patch_embed.seq.0.bn.bias', 'model.image_encoder.patch_embed.seq.0.bn.running_mean', 'model.image_encoder.patch_embed.seq.0.bn.running_var', 'model.image_encoder.patch_embed.seq.0.bn.num_batches_tracked', 'model.image_encoder.patch_embed.seq.2.c.weight', 'model.image_encoder.patch_embed.seq.2.bn.weight', 'model.image_encoder.patch_embed.seq.2.bn.bias', 'model.image_encoder.patch_embed.seq.2.bn.running_mean', 'model.image_encoder.patch_embed.seq.2.bn.running_var', 'model.image_encoder.patch_embed.seq.2.bn.num_batches_tracked', 'model.image_encoder.layers.0.blocks.0.conv1.c.weight', 'model.image_encoder.layers.0.blocks.0.conv1.bn.weight', 'model.image_encoder.layers.0.blocks.0.conv1.bn.bias', 'model.image_encoder.layers.0.blocks.0.conv1.bn.running_mean', 'model.image_encoder.layers.0.blocks.0.conv1.bn.running_var', 'model.image_encoder.layers.0.blocks.0.conv1.bn.num_batches_tracked', 'model.image_encoder.layers.0.blocks.0.conv2.c.weight', 'model.image_encoder.layers.0.blocks.0.conv2.bn.weight', 'model.image_encoder.layers.0.blocks.0.conv2.bn.bias', 'model.image_encoder.layers.0.blocks.0.conv2.bn.running_mean', 'model.image_encoder.layers.0.blocks.0.conv2.bn.running_var', 'model.image_encoder.layers.0.blocks.0.conv2.bn.num_batches_tracked', 'model.image_encoder.layers.0.blocks.0.conv3.c.weight', 'model.image_encoder.layers.0.blocks.0.conv3.bn.weight', 'model.image_encoder.layers.0.blocks.0.conv3.bn.bias', 'model.image_encoder.layers.0.blocks.0.conv3.bn.running_mean', 'model.image_encoder.layers.0.blocks.0.conv3.bn.running_var', 'model.image_encoder.layers.0.blocks.0.conv3.bn.num_batches_tracked', 'model.image_encoder.layers.0.blocks.1.conv1.c.weight', 'model.image_encoder.layers.0.blocks.1.conv1.bn.weight', 'model.image_encoder.layers.0.blocks.1.conv1.bn.bias', 'model.image_encoder.layers.0.blocks.1.conv1.bn.running_mean', 'model.image_encoder.layers.0.blocks.1.conv1.bn.running_var', 'model.image_encoder.layers.0.blocks.1.conv1.bn.num_batches_tracked', 'model.image_encoder.layers.0.blocks.1.conv2.c.weight', 'model.image_encoder.layers.0.blocks.1.conv2.bn.weight', 'model.image_encoder.layers.0.blocks.1.conv2.bn.bias', 'model.image_encoder.layers.0.blocks.1.conv2.bn.running_mean', 'model.image_encoder.layers.0.blocks.1.conv2.bn.running_var', 'model.image_encoder.layers.0.blocks.1.conv2.bn.num_batches_tracked', 'model.image_encoder.layers.0.blocks.1.conv3.c.weight', 'model.image_encoder.layers.0.blocks.1.conv3.bn.weight', 'model.image_encoder.layers.0.blocks.1.conv3.bn.bias', 'model.image_encoder.layers.0.blocks.1.conv3.bn.running_mean', 'model.image_encoder.layers.0.blocks.1.conv3.bn.running_var', 'model.image_encoder.layers.0.blocks.1.conv3.bn.num_batches_tracked', 'model.image_encoder.layers.0.downsample.conv1.c.weight', 'model.image_encoder.layers.0.downsample.conv1.bn.weight', 'model.image_encoder.layers.0.downsample.conv1.bn.bias', 'model.image_encoder.layers.0.downsample.conv1.bn.running_mean', 'model.image_encoder.layers.0.downsample.conv1.bn.running_var', 'model.image_encoder.layers.0.downsample.conv1.bn.num_batches_tracked', 'model.image_encoder.layers.0.downsample.conv2.c.weight', 'model.image_encoder.layers.0.downsample.conv2.bn.weight', 'model.image_encoder.layers.0.downsample.conv2.bn.bias', 'model.image_encoder.layers.0.downsample.conv2.bn.running_mean', 'model.image_encoder.layers.0.downsample.conv2.bn.running_var', 'model.image_encoder.layers.0.downsample.conv2.bn.num_batches_tracked', 'model.image_encoder.layers.0.downsample.conv3.c.weight', 'model.image_encoder.layers.0.downsample.conv3.bn.weight', 'model.image_encoder.layers.0.downsample.conv3.bn.bias', 'model.image_encoder.layers.0.downsample.conv3.bn.running_mean', 'model.image_encoder.layers.0.downsample.conv3.bn.running_var', 'model.image_encoder.layers.0.downsample.conv3.bn.num_batches_tracked', 'model.image_encoder.layers.1.blocks.0.attn.attention_biases', 'model.image_encoder.layers.1.blocks.0.attn.norm.weight', 'model.image_encoder.layers.1.blocks.0.attn.norm.bias', 'model.image_encoder.layers.1.blocks.0.attn.qkv.weight', 'model.image_encoder.layers.1.blocks.0.attn.qkv.bias', 'model.image_encoder.layers.1.blocks.0.attn.proj.weight', 'model.image_encoder.layers.1.blocks.0.attn.proj.bias', 'model.image_encoder.layers.1.blocks.0.mlp.norm.weight', 'model.image_encoder.layers.1.blocks.0.mlp.norm.bias', 'model.image_encoder.layers.1.blocks.0.mlp.fc1.weight', 'model.image_encoder.layers.1.blocks.0.mlp.fc1.bias', 'model.image_encoder.layers.1.blocks.0.mlp.fc2.weight', 'model.image_encoder.layers.1.blocks.0.mlp.fc2.bias', 'model.image_encoder.layers.1.blocks.0.local_conv.c.weight', 'model.image_encoder.layers.1.blocks.0.local_conv.bn.weight', 'model.image_encoder.layers.1.blocks.0.local_conv.bn.bias', 'model.image_encoder.layers.1.blocks.0.local_conv.bn.running_mean', 'model.image_encoder.layers.1.blocks.0.local_conv.bn.running_var', 'model.image_encoder.layers.1.blocks.0.local_conv.bn.num_batches_tracked', 'model.image_encoder.layers.1.blocks.1.attn.attention_biases', 'model.image_encoder.layers.1.blocks.1.attn.norm.weight', 'model.image_encoder.layers.1.blocks.1.attn.norm.bias', 'model.image_encoder.layers.1.blocks.1.attn.qkv.weight', 'model.image_encoder.layers.1.blocks.1.attn.qkv.bias', 'model.image_encoder.layers.1.blocks.1.attn.proj.weight', 'model.image_encoder.layers.1.blocks.1.attn.proj.bias', 'model.image_encoder.layers.1.blocks.1.mlp.norm.weight', 'model.image_encoder.layers.1.blocks.1.mlp.norm.bias', 'model.image_encoder.layers.1.blocks.1.mlp.fc1.weight', 'model.image_encoder.layers.1.blocks.1.mlp.fc1.bias', 'model.image_encoder.layers.1.blocks.1.mlp.fc2.weight', 'model.image_encoder.layers.1.blocks.1.mlp.fc2.bias', 'model.image_encoder.layers.1.blocks.1.local_conv.c.weight', 'model.image_encoder.layers.1.blocks.1.local_conv.bn.weight', 'model.image_encoder.layers.1.blocks.1.local_conv.bn.bias', 'model.image_encoder.layers.1.blocks.1.local_conv.bn.running_mean', 'model.image_encoder.layers.1.blocks.1.local_conv.bn.running_var', 'model.image_encoder.layers.1.blocks.1.local_conv.bn.num_batches_tracked', 'model.image_encoder.layers.1.downsample.conv1.c.weight', 'model.image_encoder.layers.1.downsample.conv1.bn.weight', 'model.image_encoder.layers.1.downsample.conv1.bn.bias', 'model.image_encoder.layers.1.downsample.conv1.bn.running_mean', 'model.image_encoder.layers.1.downsample.conv1.bn.running_var', 'model.image_encoder.layers.1.downsample.conv1.bn.num_batches_tracked', 'model.image_encoder.layers.1.downsample.conv2.c.weight', 'model.image_encoder.layers.1.downsample.conv2.bn.weight', 'model.image_encoder.layers.1.downsample.conv2.bn.bias', 'model.image_encoder.layers.1.downsample.conv2.bn.running_mean', 'model.image_encoder.layers.1.downsample.conv2.bn.running_var', 'model.image_encoder.layers.1.downsample.conv2.bn.num_batches_tracked', 'model.image_encoder.layers.1.downsample.conv3.c.weight', 'model.image_encoder.layers.1.downsample.conv3.bn.weight', 'model.image_encoder.layers.1.downsample.conv3.bn.bias', 'model.image_encoder.layers.1.downsample.conv3.bn.running_mean', 'model.image_encoder.layers.1.downsample.conv3.bn.running_var', 'model.image_encoder.layers.1.downsample.conv3.bn.num_batches_tracked', 'model.image_encoder.layers.2.blocks.0.attn.attention_biases', 'model.image_encoder.layers.2.blocks.0.attn.norm.weight', 'model.image_encoder.layers.2.blocks.0.attn.norm.bias', 'model.image_encoder.layers.2.blocks.0.attn.qkv.weight', 'model.image_encoder.layers.2.blocks.0.attn.qkv.bias', 'model.image_encoder.layers.2.blocks.0.attn.proj.weight', 'model.image_encoder.layers.2.blocks.0.attn.proj.bias', 'model.image_encoder.layers.2.blocks.0.mlp.norm.weight', 'model.image_encoder.layers.2.blocks.0.mlp.norm.bias', 'model.image_encoder.layers.2.blocks.0.mlp.fc1.weight', 'model.image_encoder.layers.2.blocks.0.mlp.fc1.bias', 'model.image_encoder.layers.2.blocks.0.mlp.fc2.weight', 'model.image_encoder.layers.2.blocks.0.mlp.fc2.bias', 'model.image_encoder.layers.2.blocks.0.local_conv.c.weight', 'model.image_encoder.layers.2.blocks.0.local_conv.bn.weight', 'model.image_encoder.layers.2.blocks.0.local_conv.bn.bias', 'model.image_encoder.layers.2.blocks.0.local_conv.bn.running_mean', 'model.image_encoder.layers.2.blocks.0.local_conv.bn.running_var', 'model.image_encoder.layers.2.blocks.0.local_conv.bn.num_batches_tracked', 'model.image_encoder.layers.2.blocks.1.attn.attention_biases', 'model.image_encoder.layers.2.blocks.1.attn.norm.weight', 'model.image_encoder.layers.2.blocks.1.attn.norm.bias', 'model.image_encoder.layers.2.blocks.1.attn.qkv.weight', 'model.image_encoder.layers.2.blocks.1.attn.qkv.bias', 'model.image_encoder.layers.2.blocks.1.attn.proj.weight', 'model.image_encoder.layers.2.blocks.1.attn.proj.bias', 'model.image_encoder.layers.2.blocks.1.mlp.norm.weight', 'model.image_encoder.layers.2.blocks.1.mlp.norm.bias', 'model.image_encoder.layers.2.blocks.1.mlp.fc1.weight', 'model.image_encoder.layers.2.blocks.1.mlp.fc1.bias', 'model.image_encoder.layers.2.blocks.1.mlp.fc2.weight', 'model.image_encoder.layers.2.blocks.1.mlp.fc2.bias', 'model.image_encoder.layers.2.blocks.1.local_conv.c.weight', 'model.image_encoder.layers.2.blocks.1.local_conv.bn.weight', 'model.image_encoder.layers.2.blocks.1.local_conv.bn.bias', 'model.image_encoder.layers.2.blocks.1.local_conv.bn.running_mean', 'model.image_encoder.layers.2.blocks.1.local_conv.bn.running_var', 'model.image_encoder.layers.2.blocks.1.local_conv.bn.num_batches_tracked', 'model.image_encoder.layers.2.blocks.2.attn.attention_biases', 'model.image_encoder.layers.2.blocks.2.attn.norm.weight', 'model.image_encoder.layers.2.blocks.2.attn.norm.bias', 'model.image_encoder.layers.2.blocks.2.attn.qkv.weight', 'model.image_encoder.layers.2.blocks.2.attn.qkv.bias', 'model.image_encoder.layers.2.blocks.2.attn.proj.weight', 'model.image_encoder.layers.2.blocks.2.attn.proj.bias', 'model.image_encoder.layers.2.blocks.2.mlp.norm.weight', 'model.image_encoder.layers.2.blocks.2.mlp.norm.bias', 'model.image_encoder.layers.2.blocks.2.mlp.fc1.weight', 'model.image_encoder.layers.2.blocks.2.mlp.fc1.bias', 'model.image_encoder.layers.2.blocks.2.mlp.fc2.weight', 'model.image_encoder.layers.2.blocks.2.mlp.fc2.bias', 'model.image_encoder.layers.2.blocks.2.local_conv.c.weight', 'model.image_encoder.layers.2.blocks.2.local_conv.bn.weight', 'model.image_encoder.layers.2.blocks.2.local_conv.bn.bias', 'model.image_encoder.layers.2.blocks.2.local_conv.bn.running_mean', 'model.image_encoder.layers.2.blocks.2.local_conv.bn.running_var', 'model.image_encoder.layers.2.blocks.2.local_conv.bn.num_batches_tracked', 'model.image_encoder.layers.2.blocks.3.attn.attention_biases', 'model.image_encoder.layers.2.blocks.3.attn.norm.weight', 'model.image_encoder.layers.2.blocks.3.attn.norm.bias', 'model.image_encoder.layers.2.blocks.3.attn.qkv.weight', 'model.image_encoder.layers.2.blocks.3.attn.qkv.bias', 'model.image_encoder.layers.2.blocks.3.attn.proj.weight', 'model.image_encoder.layers.2.blocks.3.attn.proj.bias', 'model.image_encoder.layers.2.blocks.3.mlp.norm.weight', 'model.image_encoder.layers.2.blocks.3.mlp.norm.bias', 'model.image_encoder.layers.2.blocks.3.mlp.fc1.weight', 'model.image_encoder.layers.2.blocks.3.mlp.fc1.bias', 'model.image_encoder.layers.2.blocks.3.mlp.fc2.weight', 'model.image_encoder.layers.2.blocks.3.mlp.fc2.bias', 'model.image_encoder.layers.2.blocks.3.local_conv.c.weight', 'model.image_encoder.layers.2.blocks.3.local_conv.bn.weight', 'model.image_encoder.layers.2.blocks.3.local_conv.bn.bias', 'model.image_encoder.layers.2.blocks.3.local_conv.bn.running_mean', 'model.image_encoder.layers.2.blocks.3.local_conv.bn.running_var', 'model.image_encoder.layers.2.blocks.3.local_conv.bn.num_batches_tracked', 'model.image_encoder.layers.2.blocks.4.attn.attention_biases', 'model.image_encoder.layers.2.blocks.4.attn.norm.weight', 'model.image_encoder.layers.2.blocks.4.attn.norm.bias', 'model.image_encoder.layers.2.blocks.4.attn.qkv.weight', 'model.image_encoder.layers.2.blocks.4.attn.qkv.bias', 'model.image_encoder.layers.2.blocks.4.attn.proj.weight', 'model.image_encoder.layers.2.blocks.4.attn.proj.bias', 'model.image_encoder.layers.2.blocks.4.mlp.norm.weight', 'model.image_encoder.layers.2.blocks.4.mlp.norm.bias', 'model.image_encoder.layers.2.blocks.4.mlp.fc1.weight', 'model.image_encoder.layers.2.blocks.4.mlp.fc1.bias', 'model.image_encoder.layers.2.blocks.4.mlp.fc2.weight', 'model.image_encoder.layers.2.blocks.4.mlp.fc2.bias', 'model.image_encoder.layers.2.blocks.4.local_conv.c.weight', 'model.image_encoder.layers.2.blocks.4.local_conv.bn.weight', 'model.image_encoder.layers.2.blocks.4.local_conv.bn.bias', 'model.image_encoder.layers.2.blocks.4.local_conv.bn.running_mean', 'model.image_encoder.layers.2.blocks.4.local_conv.bn.running_var', 'model.image_encoder.layers.2.blocks.4.local_conv.bn.num_batches_tracked', 'model.image_encoder.layers.2.blocks.5.attn.attention_biases', 'model.image_encoder.layers.2.blocks.5.attn.norm.weight', 'model.image_encoder.layers.2.blocks.5.attn.norm.bias', 'model.image_encoder.layers.2.blocks.5.attn.qkv.weight', 'model.image_encoder.layers.2.blocks.5.attn.qkv.bias', 'model.image_encoder.layers.2.blocks.5.attn.proj.weight', 'model.image_encoder.layers.2.blocks.5.attn.proj.bias', 'model.image_encoder.layers.2.blocks.5.mlp.norm.weight', 'model.image_encoder.layers.2.blocks.5.mlp.norm.bias', 'model.image_encoder.layers.2.blocks.5.mlp.fc1.weight', 'model.image_encoder.layers.2.blocks.5.mlp.fc1.bias', 'model.image_encoder.layers.2.blocks.5.mlp.fc2.weight', 'model.image_encoder.layers.2.blocks.5.mlp.fc2.bias', 'model.image_encoder.layers.2.blocks.5.local_conv.c.weight', 'model.image_encoder.layers.2.blocks.5.local_conv.bn.weight', 'model.image_encoder.layers.2.blocks.5.local_conv.bn.bias', 'model.image_encoder.layers.2.blocks.5.local_conv.bn.running_mean', 'model.image_encoder.layers.2.blocks.5.local_conv.bn.running_var', 'model.image_encoder.layers.2.blocks.5.local_conv.bn.num_batches_tracked', 'model.image_encoder.layers.2.downsample.conv1.c.weight', 'model.image_encoder.layers.2.downsample.conv1.bn.weight', 'model.image_encoder.layers.2.downsample.conv1.bn.bias', 'model.image_encoder.layers.2.downsample.conv1.bn.running_mean', 'model.image_encoder.layers.2.downsample.conv1.bn.running_var', 'model.image_encoder.layers.2.downsample.conv1.bn.num_batches_tracked', 'model.image_encoder.layers.2.downsample.conv2.c.weight', 'model.image_encoder.layers.2.downsample.conv2.bn.weight', 'model.image_encoder.layers.2.downsample.conv2.bn.bias', 'model.image_encoder.layers.2.downsample.conv2.bn.running_mean', 'model.image_encoder.layers.2.downsample.conv2.bn.running_var', 'model.image_encoder.layers.2.downsample.conv2.bn.num_batches_tracked', 'model.image_encoder.layers.2.downsample.conv3.c.weight', 'model.image_encoder.layers.2.downsample.conv3.bn.weight', 'model.image_encoder.layers.2.downsample.conv3.bn.bias', 'model.image_encoder.layers.2.downsample.conv3.bn.running_mean', 'model.image_encoder.layers.2.downsample.conv3.bn.running_var', 'model.image_encoder.layers.2.downsample.conv3.bn.num_batches_tracked', 'model.image_encoder.layers.3.blocks.0.attn.attention_biases', 'model.image_encoder.layers.3.blocks.0.attn.norm.weight', 'model.image_encoder.layers.3.blocks.0.attn.norm.bias', 'model.image_encoder.layers.3.blocks.0.attn.qkv.weight', 'model.image_encoder.layers.3.blocks.0.attn.qkv.bias', 'model.image_encoder.layers.3.blocks.0.attn.proj.weight', 'model.image_encoder.layers.3.blocks.0.attn.proj.bias', 'model.image_encoder.layers.3.blocks.0.mlp.norm.weight', 'model.image_encoder.layers.3.blocks.0.mlp.norm.bias', 'model.image_encoder.layers.3.blocks.0.mlp.fc1.weight', 'model.image_encoder.layers.3.blocks.0.mlp.fc1.bias', 'model.image_encoder.layers.3.blocks.0.mlp.fc2.weight', 'model.image_encoder.layers.3.blocks.0.mlp.fc2.bias', 'model.image_encoder.layers.3.blocks.0.local_conv.c.weight', 'model.image_encoder.layers.3.blocks.0.local_conv.bn.weight', 'model.image_encoder.layers.3.blocks.0.local_conv.bn.bias', 'model.image_encoder.layers.3.blocks.0.local_conv.bn.running_mean', 'model.image_encoder.layers.3.blocks.0.local_conv.bn.running_var', 'model.image_encoder.layers.3.blocks.0.local_conv.bn.num_batches_tracked', 'model.image_encoder.layers.3.blocks.1.attn.attention_biases', 'model.image_encoder.layers.3.blocks.1.attn.norm.weight', 'model.image_encoder.layers.3.blocks.1.attn.norm.bias', 'model.image_encoder.layers.3.blocks.1.attn.qkv.weight', 'model.image_encoder.layers.3.blocks.1.attn.qkv.bias', 'model.image_encoder.layers.3.blocks.1.attn.proj.weight', 'model.image_encoder.layers.3.blocks.1.attn.proj.bias', 'model.image_encoder.layers.3.blocks.1.mlp.norm.weight', 'model.image_encoder.layers.3.blocks.1.mlp.norm.bias', 'model.image_encoder.layers.3.blocks.1.mlp.fc1.weight', 'model.image_encoder.layers.3.blocks.1.mlp.fc1.bias', 'model.image_encoder.layers.3.blocks.1.mlp.fc2.weight', 'model.image_encoder.layers.3.blocks.1.mlp.fc2.bias', 'model.image_encoder.layers.3.blocks.1.local_conv.c.weight', 'model.image_encoder.layers.3.blocks.1.local_conv.bn.weight', 'model.image_encoder.layers.3.blocks.1.local_conv.bn.bias', 'model.image_encoder.layers.3.blocks.1.local_conv.bn.running_mean', 'model.image_encoder.layers.3.blocks.1.local_conv.bn.running_var', 'model.image_encoder.layers.3.blocks.1.local_conv.bn.num_batches_tracked', 'model.image_encoder.norm_head.weight', 'model.image_encoder.norm_head.bias', 'model.image_encoder.head.weight', 'model.image_encoder.head.bias', 'model.image_encoder.neck.0.weight', 'model.image_encoder.neck.1.weight', 'model.image_encoder.neck.1.bias', 'model.image_encoder.neck.2.weight', 'model.image_encoder.neck.3.weight', 'model.image_encoder.neck.3.bias', 'model.prompt_encoder.pe_layer.positional_encoding_gaussian_matrix', 'model.prompt_encoder.point_embeddings.0.weight', 'model.prompt_encoder.point_embeddings.1.weight', 'model.prompt_encoder.point_embeddings.2.weight', 'model.prompt_encoder.point_embeddings.3.weight', 'model.prompt_encoder.not_a_point_embed.weight', 'model.prompt_encoder.mask_downscaling.0.weight', 'model.prompt_encoder.mask_downscaling.0.bias', 'model.prompt_encoder.mask_downscaling.1.weight', 'model.prompt_encoder.mask_downscaling.1.bias', 'model.prompt_encoder.mask_downscaling.3.weight', 'model.prompt_encoder.mask_downscaling.3.bias', 'model.prompt_encoder.mask_downscaling.4.weight', 'model.prompt_encoder.mask_downscaling.4.bias', 'model.prompt_encoder.mask_downscaling.6.weight', 'model.prompt_encoder.mask_downscaling.6.bias', 'model.prompt_encoder.no_mask_embed.weight', 'model.mask_decoder.transformer.layers.0.self_attn.q_proj.weight', 'model.mask_decoder.transformer.layers.0.self_attn.q_proj.bias', 'model.mask_decoder.transformer.layers.0.self_attn.k_proj.weight', 'model.mask_decoder.transformer.layers.0.self_attn.k_proj.bias', 'model.mask_decoder.transformer.layers.0.self_attn.v_proj.weight', 'model.mask_decoder.transformer.layers.0.self_attn.v_proj.bias', 'model.mask_decoder.transformer.layers.0.self_attn.out_proj.weight', 'model.mask_decoder.transformer.layers.0.self_attn.out_proj.bias', 'model.mask_decoder.transformer.layers.0.norm1.weight', 'model.mask_decoder.transformer.layers.0.norm1.bias', 'model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight', 'model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias', 'model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight', 'model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias', 'model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight', 'model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias', 'model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight', 'model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias', 'model.mask_decoder.transformer.layers.0.norm2.weight', 'model.mask_decoder.transformer.layers.0.norm2.bias', 'model.mask_decoder.transformer.layers.0.mlp.lin1.weight', 'model.mask_decoder.transformer.layers.0.mlp.lin1.bias', 'model.mask_decoder.transformer.layers.0.mlp.lin2.weight', 'model.mask_decoder.transformer.layers.0.mlp.lin2.bias', 'model.mask_decoder.transformer.layers.0.norm3.weight', 'model.mask_decoder.transformer.layers.0.norm3.bias', 'model.mask_decoder.transformer.layers.0.norm4.weight', 'model.mask_decoder.transformer.layers.0.norm4.bias', 'model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight', 'model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias', 'model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight', 'model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias', 'model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight', 'model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias', 'model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight', 'model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias', 'model.mask_decoder.transformer.layers.1.self_attn.q_proj.weight', 'model.mask_decoder.transformer.layers.1.self_attn.q_proj.bias', 'model.mask_decoder.transformer.layers.1.self_attn.k_proj.weight', 'model.mask_decoder.transformer.layers.1.self_attn.k_proj.bias', 'model.mask_decoder.transformer.layers.1.self_attn.v_proj.weight', 'model.mask_decoder.transformer.layers.1.self_attn.v_proj.bias', 'model.mask_decoder.transformer.layers.1.self_attn.out_proj.weight', 'model.mask_decoder.transformer.layers.1.self_attn.out_proj.bias', 'model.mask_decoder.transformer.layers.1.norm1.weight', 'model.mask_decoder.transformer.layers.1.norm1.bias', 'model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight', 'model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias', 'model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight', 'model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias', 'model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight', 'model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias', 'model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight', 'model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias', 'model.mask_decoder.transformer.layers.1.norm2.weight', 'model.mask_decoder.transformer.layers.1.norm2.bias', 'model.mask_decoder.transformer.layers.1.mlp.lin1.weight', 'model.mask_decoder.transformer.layers.1.mlp.lin1.bias', 'model.mask_decoder.transformer.layers.1.mlp.lin2.weight', 'model.mask_decoder.transformer.layers.1.mlp.lin2.bias', 'model.mask_decoder.transformer.layers.1.norm3.weight', 'model.mask_decoder.transformer.layers.1.norm3.bias', 'model.mask_decoder.transformer.layers.1.norm4.weight', 'model.mask_decoder.transformer.layers.1.norm4.bias', 'model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight', 'model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias', 'model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight', 'model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias', 'model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight', 'model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias', 'model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight', 'model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias', 'model.mask_decoder.transformer.final_attn_token_to_image.q_proj.weight', 'model.mask_decoder.transformer.final_attn_token_to_image.q_proj.bias', 'model.mask_decoder.transformer.final_attn_token_to_image.k_proj.weight', 'model.mask_decoder.transformer.final_attn_token_to_image.k_proj.bias', 'model.mask_decoder.transformer.final_attn_token_to_image.v_proj.weight', 'model.mask_decoder.transformer.final_attn_token_to_image.v_proj.bias', 'model.mask_decoder.transformer.final_attn_token_to_image.out_proj.weight', 'model.mask_decoder.transformer.final_attn_token_to_image.out_proj.bias', 'model.mask_decoder.transformer.norm_final_attn.weight', 'model.mask_decoder.transformer.norm_final_attn.bias', 'model.mask_decoder.iou_token.weight', 'model.mask_decoder.mask_tokens.weight', 'model.mask_decoder.output_upscaling.0.weight', 'model.mask_decoder.output_upscaling.0.bias', 'model.mask_decoder.output_upscaling.1.weight', 'model.mask_decoder.output_upscaling.1.bias', 'model.mask_decoder.output_upscaling.3.weight', 'model.mask_decoder.output_upscaling.3.bias', 'model.mask_decoder.output_hypernetworks_mlps.0.layers.0.weight', 'model.mask_decoder.output_hypernetworks_mlps.0.layers.0.bias', 'model.mask_decoder.output_hypernetworks_mlps.0.layers.1.weight', 'model.mask_decoder.output_hypernetworks_mlps.0.layers.1.bias', 'model.mask_decoder.output_hypernetworks_mlps.0.layers.2.weight', 'model.mask_decoder.output_hypernetworks_mlps.0.layers.2.bias', 'model.mask_decoder.output_hypernetworks_mlps.1.layers.0.weight', 'model.mask_decoder.output_hypernetworks_mlps.1.layers.0.bias', 'model.mask_decoder.output_hypernetworks_mlps.1.layers.1.weight', 'model.mask_decoder.output_hypernetworks_mlps.1.layers.1.bias', 'model.mask_decoder.output_hypernetworks_mlps.1.layers.2.weight', 'model.mask_decoder.output_hypernetworks_mlps.1.layers.2.bias', 'model.mask_decoder.output_hypernetworks_mlps.2.layers.0.weight', 'model.mask_decoder.output_hypernetworks_mlps.2.layers.0.bias', 'model.mask_decoder.output_hypernetworks_mlps.2.layers.1.weight', 'model.mask_decoder.output_hypernetworks_mlps.2.layers.1.bias', 'model.mask_decoder.output_hypernetworks_mlps.2.layers.2.weight', 'model.mask_decoder.output_hypernetworks_mlps.2.layers.2.bias', 'model.mask_decoder.output_hypernetworks_mlps.3.layers.0.weight', 'model.mask_decoder.output_hypernetworks_mlps.3.layers.0.bias', 'model.mask_decoder.output_hypernetworks_mlps.3.layers.1.weight', 'model.mask_decoder.output_hypernetworks_mlps.3.layers.1.bias', 'model.mask_decoder.output_hypernetworks_mlps.3.layers.2.weight', 'model.mask_decoder.output_hypernetworks_mlps.3.layers.2.bias', 'model.mask_decoder.iou_prediction_head.layers.0.weight', 'model.mask_decoder.iou_prediction_head.layers.0.bias', 'model.mask_decoder.iou_prediction_head.layers.1.weight', 'model.mask_decoder.iou_prediction_head.layers.1.bias', 'model.mask_decoder.iou_prediction_head.layers.2.weight', 'model.mask_decoder.iou_prediction_head.layers.2.bias'])



Checkpoint keys: odict_keys(['image_encoder.patch_embed.seq.0.c.weight', 'image_encoder.patch_embed.seq.0.bn.weight', 'image_encoder.patch_embed.seq.0.bn.bias', 'image_encoder.patch_embed.seq.0.bn.running_mean', 'image_encoder.patch_embed.seq.0.bn.running_var', 'image_encoder.patch_embed.seq.0.bn.num_batches_tracked', 'image_encoder.patch_embed.seq.2.c.weight', 'image_encoder.patch_embed.seq.2.bn.weight', 'image_encoder.patch_embed.seq.2.bn.bias', 'image_encoder.patch_embed.seq.2.bn.running_mean', 'image_encoder.patch_embed.seq.2.bn.running_var', 'image_encoder.patch_embed.seq.2.bn.num_batches_tracked', 'image_encoder.layers.0.blocks.0.conv1.c.weight', 'image_encoder.layers.0.blocks.0.conv1.bn.weight', 'image_encoder.layers.0.blocks.0.conv1.bn.bias', 'image_encoder.layers.0.blocks.0.conv1.bn.running_mean', 'image_encoder.layers.0.blocks.0.conv1.bn.running_var', 'image_encoder.layers.0.blocks.0.conv1.bn.num_batches_tracked', 'image_encoder.layers.0.blocks.0.conv2.c.weight', 'image_encoder.layers.0.blocks.0.conv2.bn.weight', 'image_encoder.layers.0.blocks.0.conv2.bn.bias', 'image_encoder.layers.0.blocks.0.conv2.bn.running_mean', 'image_encoder.layers.0.blocks.0.conv2.bn.running_var', 'image_encoder.layers.0.blocks.0.conv2.bn.num_batches_tracked', 'image_encoder.layers.0.blocks.0.conv3.c.weight', 'image_encoder.layers.0.blocks.0.conv3.bn.weight', 'image_encoder.layers.0.blocks.0.conv3.bn.bias', 'image_encoder.layers.0.blocks.0.conv3.bn.running_mean', 'image_encoder.layers.0.blocks.0.conv3.bn.running_var', 'image_encoder.layers.0.blocks.0.conv3.bn.num_batches_tracked', 'image_encoder.layers.0.blocks.1.conv1.c.weight', 'image_encoder.layers.0.blocks.1.conv1.bn.weight', 'image_encoder.layers.0.blocks.1.conv1.bn.bias', 'image_encoder.layers.0.blocks.1.conv1.bn.running_mean', 'image_encoder.layers.0.blocks.1.conv1.bn.running_var', 'image_encoder.layers.0.blocks.1.conv1.bn.num_batches_tracked', 'image_encoder.layers.0.blocks.1.conv2.c.weight', 'image_encoder.layers.0.blocks.1.conv2.bn.weight', 'image_encoder.layers.0.blocks.1.conv2.bn.bias', 'image_encoder.layers.0.blocks.1.conv2.bn.running_mean', 'image_encoder.layers.0.blocks.1.conv2.bn.running_var', 'image_encoder.layers.0.blocks.1.conv2.bn.num_batches_tracked', 'image_encoder.layers.0.blocks.1.conv3.c.weight', 'image_encoder.layers.0.blocks.1.conv3.bn.weight', 'image_encoder.layers.0.blocks.1.conv3.bn.bias', 'image_encoder.layers.0.blocks.1.conv3.bn.running_mean', 'image_encoder.layers.0.blocks.1.conv3.bn.running_var', 'image_encoder.layers.0.blocks.1.conv3.bn.num_batches_tracked', 'image_encoder.layers.0.downsample.conv1.c.weight', 'image_encoder.layers.0.downsample.conv1.bn.weight', 'image_encoder.layers.0.downsample.conv1.bn.bias', 'image_encoder.layers.0.downsample.conv1.bn.running_mean', 'image_encoder.layers.0.downsample.conv1.bn.running_var', 'image_encoder.layers.0.downsample.conv1.bn.num_batches_tracked', 'image_encoder.layers.0.downsample.conv2.c.weight', 'image_encoder.layers.0.downsample.conv2.bn.weight', 'image_encoder.layers.0.downsample.conv2.bn.bias', 'image_encoder.layers.0.downsample.conv2.bn.running_mean', 'image_encoder.layers.0.downsample.conv2.bn.running_var', 'image_encoder.layers.0.downsample.conv2.bn.num_batches_tracked', 'image_encoder.layers.0.downsample.conv3.c.weight', 'image_encoder.layers.0.downsample.conv3.bn.weight', 'image_encoder.layers.0.downsample.conv3.bn.bias', 'image_encoder.layers.0.downsample.conv3.bn.running_mean', 'image_encoder.layers.0.downsample.conv3.bn.running_var', 'image_encoder.layers.0.downsample.conv3.bn.num_batches_tracked', 'image_encoder.layers.1.blocks.0.attn.attention_biases', 'image_encoder.layers.1.blocks.0.attn.norm.weight', 'image_encoder.layers.1.blocks.0.attn.norm.bias', 'image_encoder.layers.1.blocks.0.attn.qkv.weight', 'image_encoder.layers.1.blocks.0.attn.qkv.bias', 'image_encoder.layers.1.blocks.0.attn.proj.weight', 'image_encoder.layers.1.blocks.0.attn.proj.bias', 'image_encoder.layers.1.blocks.0.mlp.norm.weight', 'image_encoder.layers.1.blocks.0.mlp.norm.bias', 'image_encoder.layers.1.blocks.0.mlp.fc1.weight', 'image_encoder.layers.1.blocks.0.mlp.fc1.bias', 'image_encoder.layers.1.blocks.0.mlp.fc2.weight', 'image_encoder.layers.1.blocks.0.mlp.fc2.bias', 'image_encoder.layers.1.blocks.0.local_conv.c.weight', 'image_encoder.layers.1.blocks.0.local_conv.bn.weight', 'image_encoder.layers.1.blocks.0.local_conv.bn.bias', 'image_encoder.layers.1.blocks.0.local_conv.bn.running_mean', 'image_encoder.layers.1.blocks.0.local_conv.bn.running_var', 'image_encoder.layers.1.blocks.0.local_conv.bn.num_batches_tracked', 'image_encoder.layers.1.blocks.1.attn.attention_biases', 'image_encoder.layers.1.blocks.1.attn.norm.weight', 'image_encoder.layers.1.blocks.1.attn.norm.bias', 'image_encoder.layers.1.blocks.1.attn.qkv.weight', 'image_encoder.layers.1.blocks.1.attn.qkv.bias', 'image_encoder.layers.1.blocks.1.attn.proj.weight', 'image_encoder.layers.1.blocks.1.attn.proj.bias', 'image_encoder.layers.1.blocks.1.mlp.norm.weight', 'image_encoder.layers.1.blocks.1.mlp.norm.bias', 'image_encoder.layers.1.blocks.1.mlp.fc1.weight', 'image_encoder.layers.1.blocks.1.mlp.fc1.bias', 'image_encoder.layers.1.blocks.1.mlp.fc2.weight', 'image_encoder.layers.1.blocks.1.mlp.fc2.bias', 'image_encoder.layers.1.blocks.1.local_conv.c.weight', 'image_encoder.layers.1.blocks.1.local_conv.bn.weight', 'image_encoder.layers.1.blocks.1.local_conv.bn.bias', 'image_encoder.layers.1.blocks.1.local_conv.bn.running_mean', 'image_encoder.layers.1.blocks.1.local_conv.bn.running_var', 'image_encoder.layers.1.blocks.1.local_conv.bn.num_batches_tracked', 'image_encoder.layers.1.downsample.conv1.c.weight', 'image_encoder.layers.1.downsample.conv1.bn.weight', 'image_encoder.layers.1.downsample.conv1.bn.bias', 'image_encoder.layers.1.downsample.conv1.bn.running_mean', 'image_encoder.layers.1.downsample.conv1.bn.running_var', 'image_encoder.layers.1.downsample.conv1.bn.num_batches_tracked', 'image_encoder.layers.1.downsample.conv2.c.weight', 'image_encoder.layers.1.downsample.conv2.bn.weight', 'image_encoder.layers.1.downsample.conv2.bn.bias', 'image_encoder.layers.1.downsample.conv2.bn.running_mean', 'image_encoder.layers.1.downsample.conv2.bn.running_var', 'image_encoder.layers.1.downsample.conv2.bn.num_batches_tracked', 'image_encoder.layers.1.downsample.conv3.c.weight', 'image_encoder.layers.1.downsample.conv3.bn.weight', 'image_encoder.layers.1.downsample.conv3.bn.bias', 'image_encoder.layers.1.downsample.conv3.bn.running_mean', 'image_encoder.layers.1.downsample.conv3.bn.running_var', 'image_encoder.layers.1.downsample.conv3.bn.num_batches_tracked', 'image_encoder.layers.2.blocks.0.attn.attention_biases', 'image_encoder.layers.2.blocks.0.attn.norm.weight', 'image_encoder.layers.2.blocks.0.attn.norm.bias', 'image_encoder.layers.2.blocks.0.attn.qkv.weight', 'image_encoder.layers.2.blocks.0.attn.qkv.bias', 'image_encoder.layers.2.blocks.0.attn.proj.weight', 'image_encoder.layers.2.blocks.0.attn.proj.bias', 'image_encoder.layers.2.blocks.0.mlp.norm.weight', 'image_encoder.layers.2.blocks.0.mlp.norm.bias', 'image_encoder.layers.2.blocks.0.mlp.fc1.weight', 'image_encoder.layers.2.blocks.0.mlp.fc1.bias', 'image_encoder.layers.2.blocks.0.mlp.fc2.weight', 'image_encoder.layers.2.blocks.0.mlp.fc2.bias', 'image_encoder.layers.2.blocks.0.local_conv.c.weight', 'image_encoder.layers.2.blocks.0.local_conv.bn.weight', 'image_encoder.layers.2.blocks.0.local_conv.bn.bias', 'image_encoder.layers.2.blocks.0.local_conv.bn.running_mean', 'image_encoder.layers.2.blocks.0.local_conv.bn.running_var', 'image_encoder.layers.2.blocks.0.local_conv.bn.num_batches_tracked', 'image_encoder.layers.2.blocks.1.attn.attention_biases', 'image_encoder.layers.2.blocks.1.attn.norm.weight', 'image_encoder.layers.2.blocks.1.attn.norm.bias', 'image_encoder.layers.2.blocks.1.attn.qkv.weight', 'image_encoder.layers.2.blocks.1.attn.qkv.bias', 'image_encoder.layers.2.blocks.1.attn.proj.weight', 'image_encoder.layers.2.blocks.1.attn.proj.bias', 'image_encoder.layers.2.blocks.1.mlp.norm.weight', 'image_encoder.layers.2.blocks.1.mlp.norm.bias', 'image_encoder.layers.2.blocks.1.mlp.fc1.weight', 'image_encoder.layers.2.blocks.1.mlp.fc1.bias', 'image_encoder.layers.2.blocks.1.mlp.fc2.weight', 'image_encoder.layers.2.blocks.1.mlp.fc2.bias', 'image_encoder.layers.2.blocks.1.local_conv.c.weight', 'image_encoder.layers.2.blocks.1.local_conv.bn.weight', 'image_encoder.layers.2.blocks.1.local_conv.bn.bias', 'image_encoder.layers.2.blocks.1.local_conv.bn.running_mean', 'image_encoder.layers.2.blocks.1.local_conv.bn.running_var', 'image_encoder.layers.2.blocks.1.local_conv.bn.num_batches_tracked', 'image_encoder.layers.2.blocks.2.attn.attention_biases', 'image_encoder.layers.2.blocks.2.attn.norm.weight', 'image_encoder.layers.2.blocks.2.attn.norm.bias', 'image_encoder.layers.2.blocks.2.attn.qkv.weight', 'image_encoder.layers.2.blocks.2.attn.qkv.bias', 'image_encoder.layers.2.blocks.2.attn.proj.weight', 'image_encoder.layers.2.blocks.2.attn.proj.bias', 'image_encoder.layers.2.blocks.2.mlp.norm.weight', 'image_encoder.layers.2.blocks.2.mlp.norm.bias', 'image_encoder.layers.2.blocks.2.mlp.fc1.weight', 'image_encoder.layers.2.blocks.2.mlp.fc1.bias', 'image_encoder.layers.2.blocks.2.mlp.fc2.weight', 'image_encoder.layers.2.blocks.2.mlp.fc2.bias', 'image_encoder.layers.2.blocks.2.local_conv.c.weight', 'image_encoder.layers.2.blocks.2.local_conv.bn.weight', 'image_encoder.layers.2.blocks.2.local_conv.bn.bias', 'image_encoder.layers.2.blocks.2.local_conv.bn.running_mean', 'image_encoder.layers.2.blocks.2.local_conv.bn.running_var', 'image_encoder.layers.2.blocks.2.local_conv.bn.num_batches_tracked', 'image_encoder.layers.2.blocks.3.attn.attention_biases', 'image_encoder.layers.2.blocks.3.attn.norm.weight', 'image_encoder.layers.2.blocks.3.attn.norm.bias', 'image_encoder.layers.2.blocks.3.attn.qkv.weight', 'image_encoder.layers.2.blocks.3.attn.qkv.bias', 'image_encoder.layers.2.blocks.3.attn.proj.weight', 'image_encoder.layers.2.blocks.3.attn.proj.bias', 'image_encoder.layers.2.blocks.3.mlp.norm.weight', 'image_encoder.layers.2.blocks.3.mlp.norm.bias', 'image_encoder.layers.2.blocks.3.mlp.fc1.weight', 'image_encoder.layers.2.blocks.3.mlp.fc1.bias', 'image_encoder.layers.2.blocks.3.mlp.fc2.weight', 'image_encoder.layers.2.blocks.3.mlp.fc2.bias', 'image_encoder.layers.2.blocks.3.local_conv.c.weight', 'image_encoder.layers.2.blocks.3.local_conv.bn.weight', 'image_encoder.layers.2.blocks.3.local_conv.bn.bias', 'image_encoder.layers.2.blocks.3.local_conv.bn.running_mean', 'image_encoder.layers.2.blocks.3.local_conv.bn.running_var', 'image_encoder.layers.2.blocks.3.local_conv.bn.num_batches_tracked', 'image_encoder.layers.2.blocks.4.attn.attention_biases', 'image_encoder.layers.2.blocks.4.attn.norm.weight', 'image_encoder.layers.2.blocks.4.attn.norm.bias', 'image_encoder.layers.2.blocks.4.attn.qkv.weight', 'image_encoder.layers.2.blocks.4.attn.qkv.bias', 'image_encoder.layers.2.blocks.4.attn.proj.weight', 'image_encoder.layers.2.blocks.4.attn.proj.bias', 'image_encoder.layers.2.blocks.4.mlp.norm.weight', 'image_encoder.layers.2.blocks.4.mlp.norm.bias', 'image_encoder.layers.2.blocks.4.mlp.fc1.weight', 'image_encoder.layers.2.blocks.4.mlp.fc1.bias', 'image_encoder.layers.2.blocks.4.mlp.fc2.weight', 'image_encoder.layers.2.blocks.4.mlp.fc2.bias', 'image_encoder.layers.2.blocks.4.local_conv.c.weight', 'image_encoder.layers.2.blocks.4.local_conv.bn.weight', 'image_encoder.layers.2.blocks.4.local_conv.bn.bias', 'image_encoder.layers.2.blocks.4.local_conv.bn.running_mean', 'image_encoder.layers.2.blocks.4.local_conv.bn.running_var', 'image_encoder.layers.2.blocks.4.local_conv.bn.num_batches_tracked', 'image_encoder.layers.2.blocks.5.attn.attention_biases', 'image_encoder.layers.2.blocks.5.attn.norm.weight', 'image_encoder.layers.2.blocks.5.attn.norm.bias', 'image_encoder.layers.2.blocks.5.attn.qkv.weight', 'image_encoder.layers.2.blocks.5.attn.qkv.bias', 'image_encoder.layers.2.blocks.5.attn.proj.weight', 'image_encoder.layers.2.blocks.5.attn.proj.bias', 'image_encoder.layers.2.blocks.5.mlp.norm.weight', 'image_encoder.layers.2.blocks.5.mlp.norm.bias', 'image_encoder.layers.2.blocks.5.mlp.fc1.weight', 'image_encoder.layers.2.blocks.5.mlp.fc1.bias', 'image_encoder.layers.2.blocks.5.mlp.fc2.weight', 'image_encoder.layers.2.blocks.5.mlp.fc2.bias', 'image_encoder.layers.2.blocks.5.local_conv.c.weight', 'image_encoder.layers.2.blocks.5.local_conv.bn.weight', 'image_encoder.layers.2.blocks.5.local_conv.bn.bias', 'image_encoder.layers.2.blocks.5.local_conv.bn.running_mean', 'image_encoder.layers.2.blocks.5.local_conv.bn.running_var', 'image_encoder.layers.2.blocks.5.local_conv.bn.num_batches_tracked', 'image_encoder.layers.2.downsample.conv1.c.weight', 'image_encoder.layers.2.downsample.conv1.bn.weight', 'image_encoder.layers.2.downsample.conv1.bn.bias', 'image_encoder.layers.2.downsample.conv1.bn.running_mean', 'image_encoder.layers.2.downsample.conv1.bn.running_var', 'image_encoder.layers.2.downsample.conv1.bn.num_batches_tracked', 'image_encoder.layers.2.downsample.conv2.c.weight', 'image_encoder.layers.2.downsample.conv2.bn.weight', 'image_encoder.layers.2.downsample.conv2.bn.bias', 'image_encoder.layers.2.downsample.conv2.bn.running_mean', 'image_encoder.layers.2.downsample.conv2.bn.running_var', 'image_encoder.layers.2.downsample.conv2.bn.num_batches_tracked', 'image_encoder.layers.2.downsample.conv3.c.weight', 'image_encoder.layers.2.downsample.conv3.bn.weight', 'image_encoder.layers.2.downsample.conv3.bn.bias', 'image_encoder.layers.2.downsample.conv3.bn.running_mean', 'image_encoder.layers.2.downsample.conv3.bn.running_var', 'image_encoder.layers.2.downsample.conv3.bn.num_batches_tracked', 'image_encoder.layers.3.blocks.0.attn.attention_biases', 'image_encoder.layers.3.blocks.0.attn.norm.weight', 'image_encoder.layers.3.blocks.0.attn.norm.bias', 'image_encoder.layers.3.blocks.0.attn.qkv.weight', 'image_encoder.layers.3.blocks.0.attn.qkv.bias', 'image_encoder.layers.3.blocks.0.attn.proj.weight', 'image_encoder.layers.3.blocks.0.attn.proj.bias', 'image_encoder.layers.3.blocks.0.mlp.norm.weight', 'image_encoder.layers.3.blocks.0.mlp.norm.bias', 'image_encoder.layers.3.blocks.0.mlp.fc1.weight', 'image_encoder.layers.3.blocks.0.mlp.fc1.bias', 'image_encoder.layers.3.blocks.0.mlp.fc2.weight', 'image_encoder.layers.3.blocks.0.mlp.fc2.bias', 'image_encoder.layers.3.blocks.0.local_conv.c.weight', 'image_encoder.layers.3.blocks.0.local_conv.bn.weight', 'image_encoder.layers.3.blocks.0.local_conv.bn.bias', 'image_encoder.layers.3.blocks.0.local_conv.bn.running_mean', 'image_encoder.layers.3.blocks.0.local_conv.bn.running_var', 'image_encoder.layers.3.blocks.0.local_conv.bn.num_batches_tracked', 'image_encoder.layers.3.blocks.1.attn.attention_biases', 'image_encoder.layers.3.blocks.1.attn.norm.weight', 'image_encoder.layers.3.blocks.1.attn.norm.bias', 'image_encoder.layers.3.blocks.1.attn.qkv.weight', 'image_encoder.layers.3.blocks.1.attn.qkv.bias', 'image_encoder.layers.3.blocks.1.attn.proj.weight', 'image_encoder.layers.3.blocks.1.attn.proj.bias', 'image_encoder.layers.3.blocks.1.mlp.norm.weight', 'image_encoder.layers.3.blocks.1.mlp.norm.bias', 'image_encoder.layers.3.blocks.1.mlp.fc1.weight', 'image_encoder.layers.3.blocks.1.mlp.fc1.bias', 'image_encoder.layers.3.blocks.1.mlp.fc2.weight', 'image_encoder.layers.3.blocks.1.mlp.fc2.bias', 'image_encoder.layers.3.blocks.1.local_conv.c.weight', 'image_encoder.layers.3.blocks.1.local_conv.bn.weight', 'image_encoder.layers.3.blocks.1.local_conv.bn.bias', 'image_encoder.layers.3.blocks.1.local_conv.bn.running_mean', 'image_encoder.layers.3.blocks.1.local_conv.bn.running_var', 'image_encoder.layers.3.blocks.1.local_conv.bn.num_batches_tracked', 'image_encoder.norm_head.weight', 'image_encoder.norm_head.bias', 'image_encoder.head.weight', 'image_encoder.head.bias', 'image_encoder.neck.0.weight', 'image_encoder.neck.1.weight', 'image_encoder.neck.1.bias', 'image_encoder.neck.2.weight', 'image_encoder.neck.3.weight', 'image_encoder.neck.3.bias', 'prompt_encoder.pe_layer.positional_encoding_gaussian_matrix', 'prompt_encoder.point_embeddings.0.weight', 'prompt_encoder.point_embeddings.1.weight', 'prompt_encoder.point_embeddings.2.weight', 'prompt_encoder.point_embeddings.3.weight', 'prompt_encoder.not_a_point_embed.weight', 'prompt_encoder.mask_downscaling.0.weight', 'prompt_encoder.mask_downscaling.0.bias', 'prompt_encoder.mask_downscaling.1.weight', 'prompt_encoder.mask_downscaling.1.bias', 'prompt_encoder.mask_downscaling.3.weight', 'prompt_encoder.mask_downscaling.3.bias', 'prompt_encoder.mask_downscaling.4.weight', 'prompt_encoder.mask_downscaling.4.bias', 'prompt_encoder.mask_downscaling.6.weight', 'prompt_encoder.mask_downscaling.6.bias', 'prompt_encoder.no_mask_embed.weight', 'mask_decoder.transformer.layers.0.self_attn.q_proj.weight', 'mask_decoder.transformer.layers.0.self_attn.q_proj.bias', 'mask_decoder.transformer.layers.0.self_attn.k_proj.weight', 'mask_decoder.transformer.layers.0.self_attn.k_proj.bias', 'mask_decoder.transformer.layers.0.self_attn.v_proj.weight', 'mask_decoder.transformer.layers.0.self_attn.v_proj.bias', 'mask_decoder.transformer.layers.0.self_attn.out_proj.weight', 'mask_decoder.transformer.layers.0.self_attn.out_proj.bias', 'mask_decoder.transformer.layers.0.norm1.weight', 'mask_decoder.transformer.layers.0.norm1.bias', 'mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight', 'mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias', 'mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight', 'mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias', 'mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight', 'mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias', 'mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight', 'mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias', 'mask_decoder.transformer.layers.0.norm2.weight', 'mask_decoder.transformer.layers.0.norm2.bias', 'mask_decoder.transformer.layers.0.mlp.lin1.weight', 'mask_decoder.transformer.layers.0.mlp.lin1.bias', 'mask_decoder.transformer.layers.0.mlp.lin2.weight', 'mask_decoder.transformer.layers.0.mlp.lin2.bias', 'mask_decoder.transformer.layers.0.norm3.weight', 'mask_decoder.transformer.layers.0.norm3.bias', 'mask_decoder.transformer.layers.0.norm4.weight', 'mask_decoder.transformer.layers.0.norm4.bias', 'mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight', 'mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias', 'mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight', 'mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias', 'mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight', 'mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias', 'mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight', 'mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias', 'mask_decoder.transformer.layers.1.self_attn.q_proj.weight', 'mask_decoder.transformer.layers.1.self_attn.q_proj.bias', 'mask_decoder.transformer.layers.1.self_attn.k_proj.weight', 'mask_decoder.transformer.layers.1.self_attn.k_proj.bias', 'mask_decoder.transformer.layers.1.self_attn.v_proj.weight', 'mask_decoder.transformer.layers.1.self_attn.v_proj.bias', 'mask_decoder.transformer.layers.1.self_attn.out_proj.weight', 'mask_decoder.transformer.layers.1.self_attn.out_proj.bias', 'mask_decoder.transformer.layers.1.norm1.weight', 'mask_decoder.transformer.layers.1.norm1.bias', 'mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight', 'mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias', 'mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight', 'mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias', 'mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight', 'mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias', 'mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight', 'mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias', 'mask_decoder.transformer.layers.1.norm2.weight', 'mask_decoder.transformer.layers.1.norm2.bias', 'mask_decoder.transformer.layers.1.mlp.lin1.weight', 'mask_decoder.transformer.layers.1.mlp.lin1.bias', 'mask_decoder.transformer.layers.1.mlp.lin2.weight', 'mask_decoder.transformer.layers.1.mlp.lin2.bias', 'mask_decoder.transformer.layers.1.norm3.weight', 'mask_decoder.transformer.layers.1.norm3.bias', 'mask_decoder.transformer.layers.1.norm4.weight', 'mask_decoder.transformer.layers.1.norm4.bias', 'mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight', 'mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias', 'mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight', 'mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias', 'mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight', 'mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias', 'mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight', 'mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias', 'mask_decoder.transformer.final_attn_token_to_image.q_proj.weight', 'mask_decoder.transformer.final_attn_token_to_image.q_proj.bias', 'mask_decoder.transformer.final_attn_token_to_image.k_proj.weight', 'mask_decoder.transformer.final_attn_token_to_image.k_proj.bias', 'mask_decoder.transformer.final_attn_token_to_image.v_proj.weight', 'mask_decoder.transformer.final_attn_token_to_image.v_proj.bias', 'mask_decoder.transformer.final_attn_token_to_image.out_proj.weight', 'mask_decoder.transformer.final_attn_token_to_image.out_proj.bias', 'mask_decoder.transformer.norm_final_attn.weight', 'mask_decoder.transformer.norm_final_attn.bias', 'mask_decoder.iou_token.weight', 'mask_decoder.mask_tokens.weight', 'mask_decoder.output_upscaling.0.weight', 'mask_decoder.output_upscaling.0.bias', 'mask_decoder.output_upscaling.1.weight', 'mask_decoder.output_upscaling.1.bias', 'mask_decoder.output_upscaling.3.weight', 'mask_decoder.output_upscaling.3.bias', 'mask_decoder.output_hypernetworks_mlps.0.layers.0.weight', 'mask_decoder.output_hypernetworks_mlps.0.layers.0.bias', 'mask_decoder.output_hypernetworks_mlps.0.layers.1.weight', 'mask_decoder.output_hypernetworks_mlps.0.layers.1.bias', 'mask_decoder.output_hypernetworks_mlps.0.layers.2.weight', 'mask_decoder.output_hypernetworks_mlps.0.layers.2.bias', 'mask_decoder.output_hypernetworks_mlps.1.layers.0.weight', 'mask_decoder.output_hypernetworks_mlps.1.layers.0.bias', 'mask_decoder.output_hypernetworks_mlps.1.layers.1.weight', 'mask_decoder.output_hypernetworks_mlps.1.layers.1.bias', 'mask_decoder.output_hypernetworks_mlps.1.layers.2.weight', 'mask_decoder.output_hypernetworks_mlps.1.layers.2.bias', 'mask_decoder.output_hypernetworks_mlps.2.layers.0.weight', 'mask_decoder.output_hypernetworks_mlps.2.layers.0.bias', 'mask_decoder.output_hypernetworks_mlps.2.layers.1.weight', 'mask_decoder.output_hypernetworks_mlps.2.layers.1.bias', 'mask_decoder.output_hypernetworks_mlps.2.layers.2.weight', 'mask_decoder.output_hypernetworks_mlps.2.layers.2.bias', 'mask_decoder.output_hypernetworks_mlps.3.layers.0.weight', 'mask_decoder.output_hypernetworks_mlps.3.layers.0.bias', 'mask_decoder.output_hypernetworks_mlps.3.layers.1.weight', 'mask_decoder.output_hypernetworks_mlps.3.layers.1.bias', 'mask_decoder.output_hypernetworks_mlps.3.layers.2.weight', 'mask_decoder.output_hypernetworks_mlps.3.layers.2.bias', 'mask_decoder.iou_prediction_head.layers.0.weight', 'mask_decoder.iou_prediction_head.layers.0.bias', 'mask_decoder.iou_prediction_head.layers.1.weight', 'mask_decoder.iou_prediction_head.layers.1.bias', 'mask_decoder.iou_prediction_head.layers.2.weight', 'mask_decoder.iou_prediction_head.layers.2.bias'])